{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146203fc",
   "metadata": {},
   "source": [
    "# NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9808a1d",
   "metadata": {},
   "source": [
    "### 1. Text to Numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5be62",
   "metadata": {},
   "source": [
    "* When dealing with text, it has to be encoded so that it can be easily processed by a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f400ffe0",
   "metadata": {},
   "source": [
    "* To encode the words, we could use their ASCII (American Standard Code for Information Interchange) values. \n",
    "![ASCII Table](https://www.johndcook.com/ascii.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65591ec",
   "metadata": {},
   "source": [
    "* However, using ASCII values limits our semantic understanding of the sentence.\n",
    "* Ex: In the below two words, we have the same letters thus having the same ASCII values but each word is having a completely opposite meaning.\n",
    "* Therefore, using ASCII values to extract meaning from the words is daunting task.\n",
    "![example 1](https://miro.medium.com/v2/resize:fit:640/format:webp/1*cUNtGgZxxyNIEtx15T1Ubw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0371461",
   "metadata": {},
   "source": [
    "* Next, instead of labelling each letter with a number (i.e. ASCII values), we can label each word.\n",
    "* Ex: In the below sentences, we have labelled each word with a number. \n",
    "![example](https://miro.medium.com/v2/resize:fit:640/format:webp/1*uVqOzeZd4q8fareUtkZkPQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e815bd6",
   "metadata": {},
   "source": [
    "* When we only view the labels and we can observe the pattern.\n",
    "![example](https://miro.medium.com/v2/resize:fit:640/format:webp/1*vY3z3R5e12fB-p-16X0e9w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a074e6",
   "metadata": {},
   "source": [
    "* Now, we can see similarity between the sentences. \n",
    "* we can begin to train a neural network which can understand the meanings of the sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf28ede",
   "metadata": {},
   "source": [
    "* When coding, we can label each word and provide a dictionary of the words being used in the sentences using the **Tokenizer**. \n",
    "* We create an instance of tokenizer and assign a hyperparameter num_words to 100. \n",
    "* This essentially takes the most common 100 words and tokenize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dade2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9889fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I Love my dog',\n",
    "    'I love my cat'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00accffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)  #The fit_on_texts() method is used to encode the sentences.\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41168e9f",
   "metadata": {},
   "source": [
    "* The **word_index** method returns a dictionary of key value pairs where the key is the word in the sentence and the value is the label assigned to it.\n",
    "* Notice that **‘I’** has been replaced by **‘i’** and both **'Love'** and **'love'** has been replaced by **'love'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "330c3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'I Love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3823ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "\n",
    "tokenizer.fit_on_texts(sentences) \n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2b5dd",
   "metadata": {},
   "source": [
    "* Tokenizer is intellegent and it ommits the punctuations when tokenizing\n",
    "* Notice, that **‘dog!’** is not treated as a separate word just because there is an exclamation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c034fdb8",
   "metadata": {},
   "source": [
    "* Passing set of sentences to the **‘texts_to_sequences()’** method converts the sentences to their labelled equivalent based on the corpus of words passed to it.\n",
    "* If the corpus has a word missing that is present in the sentence, the word while being encoded to the label equivalent is omitted and the rest of the words are encoded and printed.\n",
    "* Ex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aadf1db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 2, 4], [2, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my parrot'\n",
    "]\n",
    "seq_data = tokenizer.texts_to_sequences(test_data)\n",
    "print(seq_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd2e4af",
   "metadata": {},
   "source": [
    "* In the above test_data, the word **‘really’** is missing in the corpus. \n",
    "* Hence, while encoding, the word **‘really’** is omitted and instead the encoded sentence is **‘i love my dog’**.\n",
    "* Similarly, for the second sentence, the words **‘loves’**, **‘manatee’** is missing in the word corpus. \n",
    "* Hence, the encoded sentence is **‘my dog my’**.\n",
    "* To overcome the problem faced in the above examples, we can **either use a huge corpus of words** or use a hyperparameter **‘oov_token’** and assign it to a certain value which will be used to encode words previously unseen in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4294f9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "sentences = [\n",
    "    'I Love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences) \n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f376827",
   "metadata": {},
   "source": [
    "* Notice that **‘<00V>’** is now part of the word_index. \n",
    "* Any word not present in the sentences will replaced by the ‘<00V>’ encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dd869f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my parrot'\n",
    "]\n",
    "seq_data = tokenizer.texts_to_sequences(test_data)\n",
    "print(seq_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc8d18",
   "metadata": {},
   "source": [
    "* When feeding training data to the neural network, a **uniformity** of the data must be maintained. \n",
    "* i.e. All sentences being fed should be in similar dimensions.\n",
    "* In NLP, while feeding training data in the form of sentences, **padding** is used to provide uniformity in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "274c81bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      " padded sequences\n",
      "[[ 0  0  0  5  3  2  4]\n",
      " [ 0  0  0  5  3  2  7]\n",
      " [ 0  0  0  6  3  2  4]\n",
      " [ 8  6  9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sentences = [\n",
    "    'I Love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences) \n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences)\n",
    "\n",
    "print(\"sequences\")\n",
    "print(sequences)\n",
    "print(\"\\n padded sequences\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bee07",
   "metadata": {},
   "source": [
    "* As we can see, padding in the form of **‘00’** is generated in the beginning of the sentence. \n",
    "* Padding has been done with reference to the longest sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c951a4",
   "metadata": {},
   "source": [
    "* If padding is to be done after the sentence, the hyperparameter **padding** can be set to **‘post’**. \n",
    "* Padding is generally done with reference to the longest sentence, however the hyperparameter **maxlen** can be provided to override it and define the maximum length of the sentence. \n",
    "* However, with use of **maxlen** the information in sentences could be lost as only a certain length of the sentence is taken. \n",
    "* But you can specify from where the words are omitted. \n",
    "* Ex: Setting it to **‘post’** allows you to loose words from the end of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c26f9ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 3 2 4 0]\n",
      " [5 3 2 7 0]\n",
      " [6 3 2 4 0]\n",
      " [8 6 9 2 4]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=5)\n",
    "\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f056b",
   "metadata": {},
   "source": [
    "### 2. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d9c18",
   "metadata": {},
   "source": [
    "* Words and associated words are clustered as vectors in a multi-dimensional space. \n",
    "* This allows the Words that are present in a sentence and often words with similar meanings to be placed close to each other in the multi-dimensional space.\n",
    "* Ex: “The movie was **dull** and **boring**.”; “The movie was **fun** and **exciting**.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af3330",
   "metadata": {},
   "source": [
    "* Now imagine we pick up a vector in a higher dimensional space, suppose 16 dimensions and words that are found together are given similar vectors. \n",
    "* Overtime, words of similar meaning begin to cluster together. \n",
    "* The meaning of the words can come from labelling the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323d6fae",
   "metadata": {},
   "source": [
    "* So taking the example of the above sentence, the words dull and boring show up a lot in the negative review, therefore they have a similar sentiment and they show up close to each other in a sentence, thus their vectors will be similar. \n",
    "* As the neural network trains, it can learn these vectors and associate them with the labels to come up with something called and embedding \n",
    "* i.e. the vectors of each word with their associated sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46c0521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=120\n",
    "vocab_size=120\n",
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c41373",
   "metadata": {},
   "source": [
    "* Now while building the neural network, we use the **Embedding layer** which gives an output of the shape of a 2D array with length of the sentence as one dimension and the embedding dimension, in our case 16 as the other dimension.\n",
    "\n",
    "* Therefore, we use the **Flatten layer** just as we used it in computer vision problems. \n",
    "* In CNN based problems, a 2D array of pixels was needed to be flattened to feed it to the neural network. \n",
    "* In a NLP based problem the 2D array of Embiddings is needed to be flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd489509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 120, 16)           1920      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 1920)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 11526     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,453\n",
      "Trainable params: 13,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d774153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
