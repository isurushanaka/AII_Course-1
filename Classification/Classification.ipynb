{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isurushanaka/AII_Course-1/blob/main/Classification/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bf22998",
      "metadata": {
        "id": "7bf22998"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e740d7d",
      "metadata": {
        "id": "3e740d7d"
      },
      "outputs": [],
      "source": [
        "# Load the Iris dataset\n",
        "X, y = load_iris(return_X_y=True, as_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3801938d",
      "metadata": {
        "id": "3801938d",
        "outputId": "abe5a2ce-55fc-4a1b-e942-a5b6a315bce2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
              "0                5.1               3.5                1.4               0.2\n",
              "1                4.9               3.0                1.4               0.2\n",
              "2                4.7               3.2                1.3               0.2\n",
              "3                4.6               3.1                1.5               0.2\n",
              "4                5.0               3.6                1.4               0.2"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb980d67",
      "metadata": {
        "id": "bb980d67",
        "outputId": "3214b9f7-0e8d-4bc8-83ce-2881ed6789bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10     0\n",
              "11     0\n",
              "12     0\n",
              "13     0\n",
              "14     0\n",
              "      ..\n",
              "145    2\n",
              "146    2\n",
              "147    2\n",
              "148    2\n",
              "149    2\n",
              "Name: target, Length: 140, dtype: int32"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.tail(-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "296f3271",
      "metadata": {
        "id": "296f3271"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033be939",
      "metadata": {
        "id": "033be939",
        "outputId": "9d9e70a8-9d9f-4e96-e671-7b9241bd3fd3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.900681</td>\n",
              "      <td>1.019004</td>\n",
              "      <td>-1.340227</td>\n",
              "      <td>-1.315444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.143017</td>\n",
              "      <td>-0.131979</td>\n",
              "      <td>-1.340227</td>\n",
              "      <td>-1.315444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.385353</td>\n",
              "      <td>0.328414</td>\n",
              "      <td>-1.397064</td>\n",
              "      <td>-1.315444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.506521</td>\n",
              "      <td>0.098217</td>\n",
              "      <td>-1.283389</td>\n",
              "      <td>-1.315444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.021849</td>\n",
              "      <td>1.249201</td>\n",
              "      <td>-1.340227</td>\n",
              "      <td>-1.315444</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
              "0          -0.900681          1.019004          -1.340227         -1.315444\n",
              "1          -1.143017         -0.131979          -1.340227         -1.315444\n",
              "2          -1.385353          0.328414          -1.397064         -1.315444\n",
              "3          -1.506521          0.098217          -1.283389         -1.315444\n",
              "4          -1.021849          1.249201          -1.340227         -1.315444"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X) # scaled_data is a numpy array\n",
        "\n",
        "X = pd.DataFrame(scaled_data, columns=['sepal length (cm)','sepal width (cm)', 'petal length (cm)', 'petal width (cm)'])\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0f95aa",
      "metadata": {
        "id": "ac0f95aa"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ebce835",
      "metadata": {
        "id": "3ebce835"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d0c1b9c",
      "metadata": {
        "id": "5d0c1b9c"
      },
      "source": [
        "* Logistic regression is a classification algorithm that works by finding the best-fit line to separate different classes of data points. It uses a mathematical function called the logistic function to map input features to a probability of belonging to a specific class.\n",
        "* The algorithm adjusts the line based on the training data, aiming to maximize the likelihood of correctly classifying the data points.\n",
        "* In simpler terms, logistic regression helps us predict the probability of an outcome or class based on input features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b706183",
      "metadata": {
        "id": "0b706183"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "lr_pred = lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "270d2041",
      "metadata": {
        "id": "270d2041"
      },
      "source": [
        "### Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73336eac",
      "metadata": {
        "id": "73336eac"
      },
      "source": [
        "* Decision trees are like flowcharts that make decisions by splitting the data based on features.\n",
        "* They start with a root node and make decisions by moving down the tree branches until reaching a leaf node, which represents a predicted class.\n",
        "* The algorithm selects the best feature to split the data at each node, aiming to maximize the information gain.\n",
        "* In other words, decision trees learn patterns in the data and create a set of logical rules to classify new instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6bb4264",
      "metadata": {
        "id": "e6bb4264"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create and train the decision tree model\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "dt_pred = dt.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39ee2791",
      "metadata": {
        "id": "39ee2791"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93780ac7",
      "metadata": {
        "id": "93780ac7"
      },
      "source": [
        "* Random forest is an ensemble learning algorithm that combines multiple decision trees to make predictions.\n",
        "* It creates a collection of decision trees, each trained on a random subset of the data and using a random subset of features.\n",
        "* When making predictions, each tree votes on the class, and the majority class becomes the final prediction.\n",
        "* Random forests help reduce overfitting and improve prediction accuracy by leveraging the wisdom of multiple trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d4d284",
      "metadata": {
        "id": "29d4d284"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Create and train the random forest model\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "rf_pred = rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a10612c",
      "metadata": {
        "id": "0a10612c"
      },
      "source": [
        "### Support Vector Machines (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8624d8d4",
      "metadata": {
        "id": "8624d8d4"
      },
      "source": [
        "* Support Vector Machines are powerful algorithms used for both classification and regression tasks.\n",
        "* SVM works by finding the best hyperplane that separates different classes of data with the largest margin.\n",
        "* The algorithm selects support vectors, which are data points closest to the decision boundary, to determine the optimal hyperplane.\n",
        "* SVM can handle complex data by transforming the input features into higher dimensions using a technique called the kernel trick."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c76b694",
      "metadata": {
        "id": "7c76b694"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Create and train the SVM model\n",
        "svc = SVC()\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "svc_pred = svc.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb5dffef",
      "metadata": {
        "id": "bb5dffef"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef9a499",
      "metadata": {
        "id": "5ef9a499"
      },
      "source": [
        "* Naive Bayes is a probabilistic algorithm that calculates the probability of a data point belonging to a specific class based on its features.\n",
        "* It assumes that all features are independent of each other, hence the \"naive\" assumption.\n",
        "* Naive Bayes uses Bayes' theorem to compute the probability using prior knowledge and likelihood estimates from the training data.\n",
        "* It's a fast and efficient algorithm, especially for text classification and spam filtering tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66d62c1f",
      "metadata": {
        "id": "66d62c1f"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Create and train the Naive Bayes model\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "nb_pred = nb.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57d548c3",
      "metadata": {
        "id": "57d548c3"
      },
      "source": [
        "### k-Nearest Neighbors (k-NN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9d7d96",
      "metadata": {
        "id": "fc9d7d96"
      },
      "source": [
        "* k-Nearest Neighbors is a simple yet effective algorithm for classification and regression.\n",
        "* It works by finding the k nearest data points in the training set to a new data point and assigns the majority class or average value of those neighbors as the prediction.\n",
        "* The algorithm uses distance metrics (such as Euclidean distance) to measure the similarity between data points.\n",
        "* k-NN is based on the idea that similar data points tend to belong to the same class or have similar output values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f689383",
      "metadata": {
        "id": "8f689383"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Create and train the k-NN model\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "knn_pred = knn.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "735183c3",
      "metadata": {
        "id": "735183c3"
      },
      "source": [
        "### Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044161f5",
      "metadata": {
        "id": "044161f5"
      },
      "source": [
        "* Neural networks are inspired by the human brain and are composed of interconnected nodes called neurons.\n",
        "* They can learn complex patterns by adjusting the strength of connections between neurons.\n",
        "* Each neuron receives input signals, applies a mathematical function to it, and passes the output to the next layer.\n",
        "* Deep neural networks have multiple layers, allowing them to learn hierarchical representations of the data.\n",
        "* They are widely used in tasks such as image and speech recognition, natural language processing, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74c6e4a8",
      "metadata": {
        "id": "74c6e4a8",
        "outputId": "56cf2239-d516-4f71-aca0-f3e80999660f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create and train the neural network model\n",
        "nn = MLPClassifier()\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "nn_pred = nn.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50826ac5",
      "metadata": {
        "id": "50826ac5"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838b5932",
      "metadata": {
        "id": "838b5932"
      },
      "source": [
        "#### Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c5366c",
      "metadata": {
        "id": "18c5366c"
      },
      "source": [
        "Accuracy measures the overall correctness of the predictions by calculating the ratio of correctly predicted instances to the total number of instances in the dataset. It is a simple and intuitive metric but may not be suitable for imbalanced datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f950b901",
      "metadata": {
        "id": "f950b901",
        "outputId": "3a430e35-c1a7-4c3f-c94f-0c9cdd444959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Classifier: 0.98\n",
            "Decision Trees Classifier: 0.97\n",
            "Random Forest Classifier: 0.98\n",
            "Support Vector Machine Classifier: 0.98\n",
            "Naive Bayes Classifier: 0.97\n",
            "k-Nearest Neighbors Classifier: 0.98\n",
            "Neural Networks (MLP) Classifier: 0.98\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "print(f\"Logistic Regression Classifier: {lr_accuracy:.2f}\")\n",
        "\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "print(f\"Decision Trees Classifier: {dt_accuracy:.2f}\")\n",
        "\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "print(f\"Random Forest Classifier: {rf_accuracy:.2f}\")\n",
        "\n",
        "svc_accuracy = accuracy_score(y_test, svc_pred)\n",
        "print(f\"Support Vector Machine Classifier: {svc_accuracy:.2f}\")\n",
        "\n",
        "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
        "print(f\"Naive Bayes Classifier: {nb_accuracy:.2f}\")\n",
        "\n",
        "knn_accuracy = accuracy_score(y_test, knn_pred)\n",
        "print(f\"k-Nearest Neighbors Classifier: {knn_accuracy:.2f}\")\n",
        "\n",
        "nn_accuracy = accuracy_score(y_test, nn_pred)\n",
        "print(f\"Neural Networks (MLP) Classifier: {nn_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b52d304f",
      "metadata": {
        "id": "b52d304f"
      },
      "source": [
        "#### Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14adf8d4",
      "metadata": {
        "id": "14adf8d4"
      },
      "source": [
        "Precision measures the proportion of correctly predicted positive instances (true positives) out of all instances predicted as positive. It focuses on the quality of positive predictions and is useful when the cost of false positives is high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96094592",
      "metadata": {
        "id": "96094592",
        "outputId": "7e0a4a02-5cfe-4fe0-f4ea-09a17cc49efa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Classifier: 0.98\n",
            "Decision Trees Classifier: 0.97\n",
            "Random Forest Classifier: 0.98\n",
            "Support Vector Machine Classifier: 0.98\n",
            "Naive Bayes Classifier: 0.97\n",
            "k-Nearest Neighbors Classifier: 0.98\n",
            "Neural Networks (MLP) Classifier: 0.98\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "\n",
        "lr_precision = precision_score(y_test, lr_pred, average='micro')\n",
        "print(f\"Logistic Regression Classifier: {lr_precision:.2f}\")\n",
        "\n",
        "dt_precision = precision_score(y_test, dt_pred, average='micro')\n",
        "print(f\"Decision Trees Classifier: {dt_precision:.2f}\")\n",
        "\n",
        "rf_precision = precision_score(y_test, rf_pred, average='micro')\n",
        "print(f\"Random Forest Classifier: {rf_precision:.2f}\")\n",
        "\n",
        "svc_precision = precision_score(y_test, svc_pred, average='micro')\n",
        "print(f\"Support Vector Machine Classifier: {svc_precision:.2f}\")\n",
        "\n",
        "nb_precision = precision_score(y_test, nb_pred, average='micro')\n",
        "print(f\"Naive Bayes Classifier: {nb_precision:.2f}\")\n",
        "\n",
        "knn_precision = precision_score(y_test, knn_pred, average='micro')\n",
        "print(f\"k-Nearest Neighbors Classifier: {knn_precision:.2f}\")\n",
        "\n",
        "nn_precision = precision_score(y_test, nn_pred, average='micro')\n",
        "print(f\"Neural Networks (MLP) Classifier: {nn_precision:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffc99ecb",
      "metadata": {
        "id": "ffc99ecb"
      },
      "source": [
        "#### Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca04e185",
      "metadata": {
        "id": "ca04e185"
      },
      "source": [
        "Recall, also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances out of all actual positive instances in the dataset. It focuses on capturing all positive instances and is useful when the cost of false negatives is high."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd128803",
      "metadata": {
        "id": "fd128803",
        "outputId": "671ad1d6-de56-4c89-a624-00c88c8024aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Classifier: 0.98\n",
            "Decision Trees Classifier: 0.97\n",
            "Random Forest Classifier: 0.98\n",
            "Support Vector Machine Classifier: 0.98\n",
            "Naive Bayes Classifier: 0.97\n",
            "k-Nearest Neighbors Classifier: 0.98\n",
            "Neural Networks (MLP) Classifier: 0.98\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "\n",
        "lr_recall = recall_score(y_test, lr_pred, average='micro')\n",
        "print(f\"Logistic Regression Classifier: {lr_recall:.2f}\")\n",
        "\n",
        "dt_recall = recall_score(y_test, dt_pred, average='micro')\n",
        "print(f\"Decision Trees Classifier: {dt_recall:.2f}\")\n",
        "\n",
        "rf_recall = recall_score(y_test, rf_pred, average='micro')\n",
        "print(f\"Random Forest Classifier: {rf_recall:.2f}\")\n",
        "\n",
        "svc_recall = recall_score(y_test, svc_pred, average='micro')\n",
        "print(f\"Support Vector Machine Classifier: {svc_recall:.2f}\")\n",
        "\n",
        "nb_recall = recall_score(y_test, nb_pred, average='micro')\n",
        "print(f\"Naive Bayes Classifier: {nb_recall:.2f}\")\n",
        "\n",
        "knn_recall = recall_score(y_test, knn_pred, average='micro')\n",
        "print(f\"k-Nearest Neighbors Classifier: {knn_recall:.2f}\")\n",
        "\n",
        "nn_recall = recall_score(y_test, nn_pred, average='micro')\n",
        "print(f\"Neural Networks (MLP) Classifier: {nn_recall:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3101bd6f",
      "metadata": {
        "id": "3101bd6f"
      },
      "source": [
        "#### F1 Score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a8c904",
      "metadata": {
        "id": "41a8c904"
      },
      "source": [
        "The F1 score is the harmonic mean of precision and recall. It provides a balanced measure of the classifier's performance, taking into account both precision and recall. It is useful when the class distribution is imbalanced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65d14303",
      "metadata": {
        "id": "65d14303",
        "outputId": "3cd8d5fd-86cb-472e-9780-5d2a3af89f2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Classifier: 0.98\n",
            "Decision Trees Classifier: 0.97\n",
            "Random Forest Classifier: 0.98\n",
            "Support Vector Machine Classifier: 0.98\n",
            "Naive Bayes Classifier: 0.97\n",
            "k-Nearest Neighbors Classifier: 0.98\n",
            "Neural Networks (MLP) Classifier: 0.98\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "lr_f1 = f1_score(y_test, lr_pred, average='micro')\n",
        "print(f\"Logistic Regression Classifier: {lr_f1:.2f}\")\n",
        "\n",
        "dt_f1 = f1_score(y_test, dt_pred, average='micro')\n",
        "print(f\"Decision Trees Classifier: {dt_f1:.2f}\")\n",
        "\n",
        "rf_f1 = f1_score(y_test, rf_pred, average='micro')\n",
        "print(f\"Random Forest Classifier: {rf_f1:.2f}\")\n",
        "\n",
        "svc_f1 = f1_score(y_test, svc_pred, average='micro')\n",
        "print(f\"Support Vector Machine Classifier: {svc_f1:.2f}\")\n",
        "\n",
        "nb_f1 = f1_score(y_test, nb_pred, average='micro')\n",
        "print(f\"Naive Bayes Classifier: {nb_f1:.2f}\")\n",
        "\n",
        "knn_f1 = f1_score(y_test, knn_pred, average='micro')\n",
        "print(f\"k-Nearest Neighbors Classifier: {knn_f1:.2f}\")\n",
        "\n",
        "nn_f1 = f1_score(y_test, nn_pred, average='micro')\n",
        "print(f\"Neural Networks (MLP) Classifier: {nn_f1:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of binary classification, the terms True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN) are used to describe the outcomes of a classification model. Using these terms, we can define accuracy, precision, recall, and F1 score as follows:\n",
        "\n",
        "\n",
        "\n",
        "1.   Accuracy: Accuracy measures the overall correctness of the model's predictions and is defined as the ratio of correct predictions (TP + TN) to the total number of predictions (TP + TN + FP + FN).\n",
        "\n",
        "  Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "2.   Precision: Precision measures the proportion of correctly predicted positive instances (TP) out of the total instances predicted as positive (TP + FP). It focuses on the quality of positive predictions.\n",
        "\n",
        "  Precision = TP / (TP + FP)\n",
        "\n",
        "3.   Recall (Sensitivity or True Positive Rate): Recall measures the proportion of correctly predicted positive instances (TP) out of the actual positive instances (TP + FN). It focuses on the coverage of positive instances.\n",
        "\n",
        "  Recall = TP / (TP + FN)\n",
        "\n",
        "4.   F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a single metric that combines both precision and recall, giving equal weight to both. It is often used when there is an uneven class distribution.\n",
        "\n",
        "  F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n"
      ],
      "metadata": {
        "id": "ri8vNYNX4iky"
      },
      "id": "ri8vNYNX4iky"
    },
    {
      "cell_type": "markdown",
      "id": "1e1ca870",
      "metadata": {
        "id": "1e1ca870"
      },
      "source": [
        "#### Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e8f8b38",
      "metadata": {
        "id": "3e8f8b38"
      },
      "source": [
        "A confusion matrix provides a tabular representation of the classifier's predictions, comparing the predicted labels with the true labels. It shows the number of true positives, true negatives, false positives, and false negatives, allowing for a more detailed analysis of the classifier's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce40b340",
      "metadata": {
        "id": "ce40b340",
        "outputId": "1a2e6dc1-a4f0-491f-884a-86a7ba547030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Classifier:\n",
            " [[23  0  0]\n",
            " [ 0 19  0]\n",
            " [ 0  1 17]]\n",
            "Decision Trees Classifier:\n",
            " [[23  0  0]\n",
            " [ 0 18  1]\n",
            " [ 0  1 17]]\n",
            "Random Forest Classifier:\n",
            " [[23  0  0]\n",
            " [ 0 19  0]\n",
            " [ 0  1 17]]\n",
            "Support Vector Machine Classifier:\n",
            " [[23  0  0]\n",
            " [ 0 19  0]\n",
            " [ 0  1 17]]\n",
            "Naive Bayes Classifier:\n",
            " [[23  0  0]\n",
            " [ 0 18  1]\n",
            " [ 0  1 17]]\n",
            "k-Nearest Neighbors Classifier:\n",
            " [[23  0  0]\n",
            " [ 0 19  0]\n",
            " [ 0  1 17]]\n",
            "Neural Networks (MLP) Classifier:\n",
            " [[23  0  0]\n",
            " [ 0 19  0]\n",
            " [ 0  1 17]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "lr_confusion = confusion_matrix(y_test, lr_pred)\n",
        "print(f\"Logistic Regression Classifier:\\n {lr_confusion}\")\n",
        "\n",
        "dt_confusion = confusion_matrix(y_test, dt_pred)\n",
        "print(f\"Decision Trees Classifier:\\n {dt_confusion}\")\n",
        "\n",
        "rf_confusion = confusion_matrix(y_test, rf_pred)\n",
        "print(f\"Random Forest Classifier:\\n {rf_confusion}\")\n",
        "\n",
        "svc_confusion = confusion_matrix(y_test, svc_pred)\n",
        "print(f\"Support Vector Machine Classifier:\\n {svc_confusion}\")\n",
        "\n",
        "nb_confusion = confusion_matrix(y_test, nb_pred)\n",
        "print(f\"Naive Bayes Classifier:\\n {nb_confusion}\")\n",
        "\n",
        "knn_confusion = confusion_matrix(y_test, knn_pred)\n",
        "print(f\"k-Nearest Neighbors Classifier:\\n {knn_confusion}\")\n",
        "\n",
        "nn_confusion = confusion_matrix(y_test, nn_pred)\n",
        "print(f\"Neural Networks (MLP) Classifier:\\n {nn_confusion}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec42f5e",
      "metadata": {
        "id": "2ec42f5e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}